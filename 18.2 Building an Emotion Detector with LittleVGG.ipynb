{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LittleVGG for Emotion Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Emotion Detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28273 images belonging to 6 classes.\n",
      "Found 3534 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "\n",
    "num_classes = 6\n",
    "img_rows, img_cols = 48, 48\n",
    "batch_size = 16\n",
    "\n",
    "train_data_dir = './fer2013/train'\n",
    "validation_data_dir = './fer2013/validation'\n",
    "\n",
    "# Let's use some data augmentaiton \n",
    "train_datagen = ImageDataGenerator(\n",
    "      rescale=1./255,\n",
    "      rotation_range=30,\n",
    "      shear_range=0.3,\n",
    "      zoom_range=0.3,\n",
    "      width_shift_range=0.4,\n",
    "      height_shift_range=0.4,\n",
    "      horizontal_flip=True,\n",
    "      fill_mode='nearest')\n",
    " \n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    " \n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True)\n",
    " \n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Keras Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.layers.core import Activation, Flatten, Dropout, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras LittleVGG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 48, 48, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 48, 48, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 48, 48, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 48, 48, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 48, 48, 32)        128       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 24, 24, 64)        36928     \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 24, 24, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 24, 24, 64)        256       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 12, 12, 128)       73856     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 12, 12, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 12, 12, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 12, 12, 128)       512       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 6, 6, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 6, 6, 256)         295168    \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 6, 6, 256)         590080    \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 6, 6, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 6, 6, 256)         1024      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 3, 3, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                147520    \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 390       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 6)                 0         \n",
      "=================================================================\n",
      "Total params: 1,328,102\n",
      "Trainable params: 1,325,926\n",
      "Non-trainable params: 2,176\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), padding = 'same', kernel_initializer=\"he_normal\",\n",
    "                 input_shape = (img_rows, img_cols, 1)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(32, (3, 3), padding = \"same\", kernel_initializer=\"he_normal\", \n",
    "                 input_shape = (img_rows, img_cols, 1)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block #2: second CONV => RELU => CONV => RELU => POOL\n",
    "# layer set\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block #3: third CONV => RELU => CONV => RELU => POOL\n",
    "# layer set\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block #4: third CONV => RELU => CONV => RELU => POOL\n",
    "# layer set\n",
    "model.add(Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(256, (3, 3), padding=\"same\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Block #5: first set of FC => RELU layers\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Block #6: second set of FC => RELU layers\n",
    "model.add(Dense(64, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Block #7: softmax classifier\n",
    "model.add(Dense(num_classes, kernel_initializer=\"he_normal\"))\n",
    "model.add(Activation(\"softmax\"))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1767/1767 [==============================] - 472s 267ms/step - loss: 1.9716 - accuracy: 0.2067 - val_loss: 1.7316 - val_accuracy: 0.2491\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.73159, saving model to /home/rd/Documents/Projects/DeepLearningCV2/DeepLearningCV/Emotion_Age_Gender/emotion_little_vgg_3.h5\n",
      "Epoch 2/10\n",
      "1767/1767 [==============================] - 442s 250ms/step - loss: 1.7436 - accuracy: 0.2496 - val_loss: 1.8372 - val_accuracy: 0.2607\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 1.73159\n",
      "Epoch 3/10\n",
      "1767/1767 [==============================] - 433s 245ms/step - loss: 1.7260 - accuracy: 0.2630 - val_loss: 1.5720 - val_accuracy: 0.2794\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.73159 to 1.57197, saving model to /home/rd/Documents/Projects/DeepLearningCV2/DeepLearningCV/Emotion_Age_Gender/emotion_little_vgg_3.h5\n",
      "Epoch 4/10\n",
      "1767/1767 [==============================] - 440s 249ms/step - loss: 1.6930 - accuracy: 0.2825 - val_loss: 1.3811 - val_accuracy: 0.3491\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.57197 to 1.38114, saving model to /home/rd/Documents/Projects/DeepLearningCV2/DeepLearningCV/Emotion_Age_Gender/emotion_little_vgg_3.h5\n",
      "Epoch 5/10\n",
      "1767/1767 [==============================] - 432s 244ms/step - loss: 1.6392 - accuracy: 0.3172 - val_loss: 1.7173 - val_accuracy: 0.3991\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 1.38114\n",
      "Epoch 6/10\n",
      "1767/1767 [==============================] - 488s 276ms/step - loss: 1.5509 - accuracy: 0.3730 - val_loss: 1.4168 - val_accuracy: 0.3974\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 1.38114\n",
      "Epoch 7/10\n",
      "1767/1767 [==============================] - 494s 280ms/step - loss: 1.4939 - accuracy: 0.4026 - val_loss: 1.6505 - val_accuracy: 0.4548\n",
      "Restoring model weights from the end of the best epoch\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.38114\n",
      "\n",
      "Epoch 00007: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "Epoch 00007: early stopping\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "                     \n",
    "checkpoint = ModelCheckpoint(\"/home/rd/Documents/Projects/DeepLearningCV2/DeepLearningCV/Emotion_Age_Gender/emotion_little_vgg_3.h5\",\n",
    "                             monitor=\"val_loss\",\n",
    "                             mode=\"min\",\n",
    "                             save_best_only = True,\n",
    "                             verbose=1)\n",
    "\n",
    "earlystop = EarlyStopping(monitor = 'val_loss', \n",
    "                          min_delta = 0, \n",
    "                          patience = 3,\n",
    "                          verbose = 1,\n",
    "                          restore_best_weights = True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, patience = 3, verbose = 1, min_delta = 0.0001)\n",
    "\n",
    "# we put our call backs into a callback list\n",
    "callbacks = [earlystop, checkpoint, reduce_lr]\n",
    "\n",
    "# We use a very small learning rate \n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = Adam(lr=0.001),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "nb_train_samples = 28273\n",
    "nb_validation_samples = 3534\n",
    "epochs = 10\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = nb_train_samples // batch_size,\n",
    "    epochs = epochs,\n",
    "    callbacks = callbacks,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps = nb_validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3534 images belonging to 6 classes.\n",
      "Confusion Matrix\n",
      "[[ 20  14 269   1 133  54]\n",
      " [ 14  28 260   5  84 137]\n",
      " [  7  18 711   0  85  58]\n",
      " [ 14  24 346  10 113 119]\n",
      " [ 15  11 369   4 166  29]\n",
      " [  3  23  60   6  23 301]]\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Angry       0.27      0.04      0.07       491\n",
      "        Fear       0.24      0.05      0.09       528\n",
      "       Happy       0.35      0.81      0.49       879\n",
      "     Neutral       0.38      0.02      0.03       626\n",
      "         Sad       0.27      0.28      0.28       594\n",
      "    Surprise       0.43      0.72      0.54       416\n",
      "\n",
      "    accuracy                           0.35      3534\n",
      "   macro avg       0.33      0.32      0.25      3534\n",
      "weighted avg       0.33      0.35      0.26      3534\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "nb_train_samples = 28273\n",
    "nb_validation_samples = 3534\n",
    "\n",
    "# We need to recreate our validation generator with shuffle = false\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "class_labels = validation_generator.class_indices\n",
    "class_labels = {v: k for k, v in class_labels.items()}\n",
    "classes = list(class_labels.values())\n",
    "\n",
    "#Confution Matrix and Classification Report\n",
    "Y_pred = model.predict_generator(validation_generator, nb_validation_samples // batch_size+1)\n",
    "y_pred = np.argmax(Y_pred, axis=1)\n",
    "\n",
    "print('Confusion Matrix')\n",
    "print(confusion_matrix(validation_generator.classes, y_pred))\n",
    "print('Classification Report')\n",
    "target_names = list(class_labels.values())\n",
    "print(classification_report(validation_generator.classes, y_pred, target_names=target_names))\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "cnf_matrix = confusion_matrix(validation_generator.classes, y_pred)\n",
    "\n",
    "plt.imshow(cnf_matrix, interpolation='nearest')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(classes))\n",
    "_ = plt.xticks(tick_marks, classes, rotation=90)\n",
    "_ = plt.yticks(tick_marks, classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading our saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "classifier = load_model('/home/rd/Documents/Projects/DeepLearningCV2/DeepLearningCV/Emotion_Age_Gender/emotion_little_vgg_3.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get our class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3534 images belonging to 6 classes.\n",
      "{0: 'Angry', 1: 'Fear', 2: 'Happy', 3: 'Neutral', 4: 'Sad', 5: 'Surprise'}\n"
     ]
    }
   ],
   "source": [
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        color_mode = 'grayscale',\n",
    "        target_size=(img_rows, img_cols),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False)\n",
    "\n",
    "class_labels = validation_generator.class_indices\n",
    "class_labels = {v: k for k, v in class_labels.items()}\n",
    "classes = list(class_labels.values())\n",
    "print(class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's test on some of validation images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rd/anaconda3/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:104: UserWarning: grayscale is deprecated. Please use color_mode = \"grayscale\"\n",
      "  warnings.warn('grayscale is deprecated. Please use '\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "from keras.optimizers import RMSprop, SGD, Adam\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import re\n",
    "\n",
    "def draw_test(name, pred, im, true_label):\n",
    "    BLACK = [0,0,0]\n",
    "    expanded_image = cv2.copyMakeBorder(im, 160, 0, 0, 300 ,cv2.BORDER_CONSTANT,value=BLACK)\n",
    "    cv2.putText(expanded_image, \"predicted - \"+ pred, (20, 60) , cv2.FONT_HERSHEY_SIMPLEX,1, (0,0,255), 2)\n",
    "    cv2.putText(expanded_image, \"true - \"+ true_label, (20, 120) , cv2.FONT_HERSHEY_SIMPLEX,1, (0,255,0), 2)\n",
    "    cv2.imshow(name, expanded_image)\n",
    "\n",
    "\n",
    "def getRandomImage(path, img_width, img_height):\n",
    "    \"\"\"function loads a random images from a random folder in our test path \"\"\"\n",
    "    folders = list(filter(lambda x: os.path.isdir(os.path.join(path, x)), os.listdir(path)))\n",
    "    random_directory = np.random.randint(0,len(folders))\n",
    "    path_class = folders[random_directory]\n",
    "    file_path = path + path_class\n",
    "    file_names = [f for f in listdir(file_path) if isfile(join(file_path, f))]\n",
    "    random_file_index = np.random.randint(0,len(file_names))\n",
    "    image_name = file_names[random_file_index]\n",
    "    final_path = file_path + \"/\" + image_name\n",
    "    return image.load_img(final_path, target_size = (img_width, img_height),grayscale=True), final_path, path_class\n",
    "\n",
    "# dimensions of our images\n",
    "img_width, img_height = 48, 48\n",
    "\n",
    "# We use a very small learning rate \n",
    "model.compile(loss = 'categorical_crossentropy',\n",
    "              optimizer = RMSprop(lr = 0.001),\n",
    "              metrics = ['accuracy'])\n",
    "\n",
    "files = []\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "# predicting images\n",
    "for i in range(0, 10):\n",
    "    path = './fer2013/validation/' \n",
    "    img, final_path, true_label = getRandomImage(path, img_width, img_height)\n",
    "    files.append(final_path)\n",
    "    true_labels.append(true_label)\n",
    "    x = image.img_to_array(img)\n",
    "    x = x * 1./255\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    images = np.vstack([x])\n",
    "    classes = model.predict_classes(images, batch_size = 10)\n",
    "    predictions.append(classes)\n",
    "    \n",
    "for i in range(0, len(files)):\n",
    "    image = cv2.imread((files[i]))\n",
    "    image = cv2.resize(image, None, fx=3, fy=3, interpolation = cv2.INTER_CUBIC)\n",
    "    draw_test(\"Prediction\", class_labels[predictions[i][0]], image, true_labels[i])\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "face_classifier = cv2.CascadeClassifier('./Haarcascades/haarcascade_frontalface_default.xml')\n",
    "\n",
    "def face_detector(img):\n",
    "    # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(img.copy(),cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "    if faces is ():\n",
    "        return (0,0,0,0), np.zeros((48,48), np.uint8), img\n",
    "    \n",
    "    allfaces = []   \n",
    "    rects = []\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        roi_gray = cv2.resize(roi_gray, (48, 48), interpolation = cv2.INTER_AREA)\n",
    "        allfaces.append(roi_gray)\n",
    "        rects.append((x,w,y,h))\n",
    "    return rects, allfaces, img\n",
    "\n",
    "img = cv2.imread(\"rajeev.jpg\")\n",
    "rects, faces, image = face_detector(img)\n",
    "\n",
    "i = 0\n",
    "for face in faces:\n",
    "    roi = face.astype(\"float\") / 255.0\n",
    "    roi = img_to_array(roi)\n",
    "    roi = np.expand_dims(roi, axis=0)\n",
    "\n",
    "    # make a prediction on the ROI, then lookup the class\n",
    "    preds = classifier.predict(roi)[0]\n",
    "    label = class_labels[preds.argmax()]   \n",
    "\n",
    "    #Overlay our detected emotion on our pic\n",
    "    label_position = (rects[i][0] + int((rects[i][1]/2)), abs(rects[i][2] - 10))\n",
    "    i =+ 1\n",
    "    cv2.putText(image, label, label_position , cv2.FONT_HERSHEY_SIMPLEX,1, (0,255,0), 2)\n",
    "    \n",
    "cv2.imshow(\"Emotion Detector\", image)\n",
    "cv2.waitKey(0)\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try this on our webcam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "face_classifier = cv2.CascadeClassifier('./Haarcascades/haarcascade_frontalface_default.xml')\n",
    "\n",
    "def face_detector(img):\n",
    "    # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.3, 5)\n",
    "    if faces is ():\n",
    "        return (0,0,0,0), np.zeros((48,48), np.uint8), img\n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "\n",
    "    try:\n",
    "        roi_gray = cv2.resize(roi_gray, (48, 48), interpolation = cv2.INTER_AREA)\n",
    "    except:\n",
    "        return (x,w,y,h), np.zeros((48,48), np.uint8), img\n",
    "    return (x,w,y,h), roi_gray, img\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "\n",
    "    ret, frame = cap.read()\n",
    "    rect, face, image = face_detector(frame)\n",
    "    if np.sum([face]) != 0.0:\n",
    "        roi = face.astype(\"float\") / 255.0\n",
    "        roi = img_to_array(roi)\n",
    "        roi = np.expand_dims(roi, axis=0)\n",
    "\n",
    "        # make a prediction on the ROI, then lookup the class\n",
    "        preds = classifier.predict(roi)[0]\n",
    "        label = class_labels[preds.argmax()]  \n",
    "        label_position = (rect[0] + int((rect[1]/2)), rect[2] + 25)\n",
    "        cv2.putText(image, label, label_position , cv2.FONT_HERSHEY_SIMPLEX,2, (0,255,0), 3)\n",
    "    else:\n",
    "        cv2.putText(image, \"No Face Found\", (20, 60) , cv2.FONT_HERSHEY_SIMPLEX,2, (0,255,0), 3)\n",
    "        \n",
    "    cv2.imshow('All', image)\n",
    "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
